"""
Optimises the hyperparameters of networks
"""
import os
import json
import logging as log
from time import time
from typing import Any

import torch
import joblib
import optuna
import optuna.visualization as vis
import numpy as np
from torch import optim, nn
from torch.utils.data import DataLoader
from netloader.utils.utils import get_device, progress_bar
from netloader.network import Network
from netloader import layers
from optuna import pruners
from plotly.graph_objs import Figure

from src.main import init
from src.utils.utils import open_config
from src.utils.clustering import CompactClusterEncoder


def _save_plots(plots_dir: str, plots: list[Figure]) -> None:
    """
    Saves plots generated from Optuna as a HTML file

    Parameters
    ----------
    plots_dir : str
        Directory to save plots
    plots : list[Figure]
        List of Plotly plots generated by Optuna
    """
    with open(f'{plots_dir}optimisations.html', 'w', encoding='utf-8') as file:
        for i, plot in enumerate(plots):
            file.write(plot.to_html(full_html=False, include_plotlyjs='cdn' if i == 0 else False))


def _save_name(num: int, study_dir: str) -> str:
    """
    Returns the name of the study file

    Parameters
    ----------
    num : int
        Number of the study
    study_dir : str
        Path to the directory to save studies

    Returns
    -------
    str
        Path to save the file
    """
    return f'{study_dir}study_{num}.pkl'


def _update_net(idx: int, latent_dim: int, name: str, nets_dir: str) -> None:
    """
    Updates the network configuration with latent dimension

    Parameters
    ----------
    idx : int
        Layer index for the latent space
    latent_dim : int
        Dimension of the latent space
    name : str
        Name of the network
    nets_dir : str
        Directory of network configurations
    """
    with open(f'{nets_dir}{name}.json', 'r', encoding='utf-8') as file:
        net_config = json.load(file)

    net_config['layers'][idx]['features'] = latent_dim

    with open(f'{nets_dir}optuna_config.json', 'w', encoding='utf-8') as file:
        json.dump(net_config, file)


def _objective(
        idx: int,
        loaders: tuple[DataLoader, DataLoader],
        trial: optuna.Trial) -> tuple[float, float]:
    """
    Optuna objective to pick hyperparameters and train the network

    Parameters
    ----------
    idx : int
        Layer index for the latent space
    loaders : tuple[DataLoader, DataLoader]
        Training and validation dataloaders
    trial : Trial
        Optuna trial

    Returns
    -------
    float
        Performance metric of the trial
    """
    epochs: int
    warmup: int
    smooth: int
    latent_dim: int
    initial_time: float
    cluster_loss: float
    learning_rate: float
    distance_loss: float
    name: str
    nets_dir: str
    losses: list[float] = []
    data: dict[str, np.ndarray]
    config: dict[str, Any]
    main_config: dict[str, Any]
    net: CompactClusterEncoder
    device: torch.device = get_device()[1]

    _, main_config = open_config('main', '../config.yaml')
    _, config = open_config('optimise', '../config.yaml')
    nets_dir = main_config['data']['network-configs-directory']
    epochs = config['optimisation']['epochs']
    warmup = config['optimisation']['warmup']
    smooth = config['optimisation']['smooth-number']
    name = config['optimisation']['network-name']
    net = torch.load(config['output']['base-network'])

    # Latent dimension
    latent_dim = trial.suggest_int('latent_dim', 2, 12)

    # Learning rate
    # learning_rate = trial.suggest_float('learning_rate', 4e-4, 5e-3, log=True)

    # Loss weighting
    # class_loss = trial.suggest_float('class_loss', 5e-2, 5, log=True)
    cluster_loss = trial.suggest_float('cluster_loss', 0.1, 10, log=True)
    # distance_loss = trial.suggest_float('distance_loss', 5e-2, 5, log=True)

    # Load network configuration file
    _update_net(idx, latent_dim, name, nets_dir)

    # Create network
    net.net = Network(
        'optuna_config',
        '../data/',
        list(loaders[1].dataset.dataset[0][2].shape),
        [len(torch.unique(loaders[1].dataset.dataset.labels))],
    )
    net._epoch = 0
    net.net.scale = nn.Parameter(torch.tensor((1.,), requires_grad=True))
    net.optimiser = optim.AdamW(net.net.parameters(), lr=3e-3)
    net.scheduler = optim.lr_scheduler.ReduceLROnPlateau(net.optimiser, factor=0.5)
    net.to(device)

    # Update network hyperparameters
    # net.class_loss = 1
    net.cluster_loss = cluster_loss
    # net.distance_loss = distance_loss

    # Train network
    for i in range(net._epoch, epochs):
        initial_time = time()

        # Training
        net.train(True)
        net._train_val(loaders[0])

        # Validation
        data = net.predict(loaders[1])
        net.losses[1].append(np.count_nonzero(
            data['targets'].squeeze() == data['preds']
        ) / len(data['targets']))

        # Update
        net._update_scheduler(metrics=net.losses[1][-1])
        net._update_epoch()
        losses.append(float(np.mean(net.losses[1][-smooth:])))
        # trial.report(losses[-1], i)
        progress_bar(
            i,
            epochs,
            text=f'Epoch {i + 1}/{epochs}\tAccuracy: {net.losses[1][-1]:.1%}\tTraining time: '
                 f'{time() - initial_time:.1f} s',
        )

        # Prune poor performing networks
        # if trial.should_prune() or (net._epoch > warmup and losses[-1] < 1 / len(net.classes)):
        #     raise optuna.TrialPruned()

        # End plateaued networks early
        if (net._epoch > net.scheduler.patience * 2 and
                losses[-net.scheduler.patience * 2] > losses[-1]):
            print('Trial plateaued, ending early...')
            break

    return losses[-1], torch.nn.MSELoss()(
        net.header['targets'](data['targets']).squeeze(),
        torch.from_numpy(data['latent'][:, 0]),
    ).item()


def main(config_path: str = '../config.yaml'):
    """
    Main function for network optimisation

    Parameters
    ----------
    config_path : string, default = '../config.yaml'
        Path to the configuration file
    """
    _, config = open_config('optimise', config_path)

    load = config['optimisation']['study-load']
    save = config['optimisation']['study-save']
    trials = config['optimisation']['trials']
    warmup = config['optimisation']['warmup']
    study_dir = config['data']['study-directory']
    plots_dir = config['output']['plots-directory']

    loaders, net, _ = init()
    net.save_path = config['output']['network']
    net._verbose = None
    torch.save(net, config['output']['base-network'])

    # Find the last checkpoint corresponding to the latent space
    for idx, module in enumerate(net.net.net[::-1]):
        if isinstance(module, layers.Checkpoint):
            idx = len(net.net.net) - 2 - idx
            break

    # Load existing study, or create new study
    if load:
        study = joblib.load(_save_name(load, study_dir))
    else:
        if os.path.exists(_save_name(save, study_dir)):
            log.getLogger(__name__).warning(f'{_save_name(save, study_dir)} already exists and '
                                            f'will be overwritten if training continues')

        study = optuna.create_study(
            study_name=f'Study {save}',
            directions=['maximize', 'minimize'],
            sampler=optuna.samplers.RandomSampler(),
            # pruner=pruners.PatientPruner(pruners.MedianPruner(n_warmup_steps=warmup), patience=2),
        )

    # Trial different network configurations
    for _ in range(len(study.trials), trials):
        study.optimize(
            lambda trial: _objective(idx, loaders, trial),
            n_trials=1,
        )
        joblib.dump(study, _save_name(save, study_dir))

    _save_plots(plots_dir, [
        vis.plot_optimization_history(
            study,
            target=lambda t: t.values[0] * 100,
            target_name='Accuracy',
        ),
        vis.plot_optimization_history(
            study,
            target=lambda t: t.values[1],
            target_name='Regression Loss',
        ),
        vis.plot_intermediate_values(study),
        vis.plot_parallel_coordinate(
            study,
            target=lambda t: t.values[0] * 100,
            target_name='Accuracy',
        ),
        vis.plot_parallel_coordinate(
            study,
            target=lambda t: t.values[1],
            target_name='Regression Loss',
        ),
        vis.plot_contour(
            study,
            target=lambda t: t.values[0] * 100,
            target_name='Accuracy',
        ),
        vis.plot_contour(
            study,
            target=lambda t: t.values[1],
            target_name='Regression Loss',
        ),
        vis.plot_slice(
            study,
            target=lambda t: t.values[0] * 100,
            target_name='Accuracy',
        ),
        vis.plot_slice(
            study,
            target=lambda t: t.values[1],
            target_name='Regression Loss',
        ),
        vis.plot_param_importances(
            study,
            target=lambda t: t.values[0] * 100,
            target_name='Accuracy',
        ),
        vis.plot_param_importances(
            study,
            target=lambda t: t.values[1],
            target_name='Regression Loss',
        ),
        vis.plot_param_importances(
            study,
            target=lambda t: t.duration.total_seconds(),
            target_name='duration',
        ),
        vis.plot_edf(
            study,
            target=lambda t: t.values[0] * 100,
            target_name='Accuracy',
        ),
        vis.plot_edf(
            study,
            target=lambda t: t.values[1],
            target_name='Regression Loss',
        ),
        vis.plot_rank(
            study,
            target=lambda t: t.values[0] * 100,
            target_name='Accuracy',
        ),
        vis.plot_rank(
            study,
            target=lambda t: t.values[1],
            target_name='Regression Loss',
        ),
        vis.plot_timeline(study),
    ])


if __name__ == '__main__':
    main()
